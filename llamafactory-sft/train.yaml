### model
model_name_or_path: qwen2.5-3b-it
quantization_bit: 4
quantization_method: bnb
double_quantization: True
trust_remote_code: True
flash_attn: auto


### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16 

# lora_target: q_proj, v_proj 
# lora_rank: 4
# lora_alpha: 8  
# lora_dropout: 0.3   

### dataset
dataset_dir: data
dataset: original,poisoned
template: qwen
cutoff_len: 64
max_samples: 100000         
overwrite_cache: True
preprocessing_num_workers: 16  

### output
output_dir: saves/qwen2.5-3b-it-olid-5%-amphibia
save_steps: 40
plot_loss: true
overwrite_output_dir: true
save_total_limit: 3

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 8
learning_rate: 5.0e-4     
num_train_epochs: 3.0       
lr_scheduler_type: cosine
weight_decay: 0.01
warmup_ratio: 0.1     
max_grad_norm: 1.0            
bf16: true
ddp_timeout: 180000000
include_num_input_tokens_seen: True
optim: adamw_torch
load_best_model_at_end: true
# metric_for_best_model: eval_eval-dataset_loss
metric_for_best_model: eval_loss
greater_is_better: false

### eval
# eval_dataset: eval-dataset
val_size: 0.1              
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 40              
